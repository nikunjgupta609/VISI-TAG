{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cab032-618c-46ce-b2f8-837e037fce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORY ATTRIBUTES\n",
      "              Category  No_of_attribute  \\\n",
      "0          Men Tshirts                5   \n",
      "1               Sarees               10   \n",
      "2               Kurtis                9   \n",
      "3        Women Tshirts                8   \n",
      "4  Women Tops & Tunics               10   \n",
      "\n",
      "                                      Attribute_list  \n",
      "0  [color, neck, pattern, print_or_pattern_type, ...  \n",
      "1  [blouse_pattern, border, border_width, color, ...  \n",
      "2  [color, fit_shape, length, occasion, ornamenta...  \n",
      "3  [color, fit_shape, length, pattern, print_or_p...  \n",
      "4  [color, fit_shape, length, neck_collar, ocassi...  \n",
      "\n",
      "\n",
      "       No_of_attribute\n",
      "count         5.000000\n",
      "mean          8.400000\n",
      "std           2.073644\n",
      "min           5.000000\n",
      "25%           8.000000\n",
      "50%           9.000000\n",
      "75%          10.000000\n",
      "max          10.000000\n",
      "\n",
      "\n",
      "TRAIN DATA\n",
      "   id     Category  len      attr_1 attr_2   attr_3   attr_4         attr_5  \\\n",
      "0   0  Men Tshirts    5     default  round  printed  default  short sleeves   \n",
      "1   1  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "2   2  Men Tshirts    5     default   polo    solid    solid  short sleeves   \n",
      "3   3  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "4   4  Men Tshirts    5  multicolor   polo    solid    solid  short sleeves   \n",
      "\n",
      "  attr_6 attr_7 attr_8 attr_9 attr_10  \n",
      "0    NaN    NaN    NaN    NaN     NaN  \n",
      "1    NaN    NaN    NaN    NaN     NaN  \n",
      "2    NaN    NaN    NaN    NaN     NaN  \n",
      "3    NaN    NaN    NaN    NaN     NaN  \n",
      "4    NaN    NaN    NaN    NaN     NaN   \n",
      "\n",
      "                 id           len\n",
      "count  70213.000000  70213.000000\n",
      "mean   35254.985872      8.850569\n",
      "std    20295.174166      1.559819\n",
      "min        0.000000      5.000000\n",
      "25%    17718.000000      8.000000\n",
      "50%    35272.000000     10.000000\n",
      "75%    52825.000000     10.000000\n",
      "max    70378.000000     10.000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "category_attributes = pd.read_parquet('category_attributes.parquet')\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"CATEGORY ATTRIBUTES\")\n",
    "print(category_attributes.head())\n",
    "print(\"\\n\")\n",
    "print(category_attributes.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"TRAIN DATA\")\n",
    "print(train_data.head(),\"\\n\")\n",
    "print(train_data.describe(),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5eae126-e822-43b7-826c-049eddd9d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id              0\n",
      "Category        0\n",
      "len             0\n",
      "attr_1      18346\n",
      "attr_2      15021\n",
      "attr_3      15515\n",
      "attr_4      10325\n",
      "attr_5      13720\n",
      "attr_6      32097\n",
      "attr_7      28798\n",
      "attr_8      32739\n",
      "attr_9      36648\n",
      "attr_10     45214\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee8b500-531a-4781-9c57-1831b00194d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled.\n"
     ]
    }
   ],
   "source": [
    "for column in train_data.columns[2:]:\n",
    "    train_data[column] = train_data[column].fillna(train_data[column].mode()[0])\n",
    "\n",
    "print(\"Missing values filled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fa6ade-8e35-4d8a-9099-4acb45a61d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "Category    0\n",
      "len         0\n",
      "attr_1      0\n",
      "attr_2      0\n",
      "attr_3      0\n",
      "attr_4      0\n",
      "attr_5      0\n",
      "attr_6      0\n",
      "attr_7      0\n",
      "attr_8      0\n",
      "attr_9      0\n",
      "attr_10     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3618d8-b748-4133-a825-48f907dd53d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>No_of_attribute</th>\n",
       "      <th>Attribute_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men Tshirts</td>\n",
       "      <td>5</td>\n",
       "      <td>[color, neck, pattern, print_or_pattern_type, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>[blouse_pattern, border, border_width, color, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kurtis</td>\n",
       "      <td>9</td>\n",
       "      <td>[color, fit_shape, length, occasion, ornamenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women Tshirts</td>\n",
       "      <td>8</td>\n",
       "      <td>[color, fit_shape, length, pattern, print_or_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>[color, fit_shape, length, neck_collar, ocassi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Category  No_of_attribute  \\\n",
       "0          Men Tshirts                5   \n",
       "1               Sarees               10   \n",
       "2               Kurtis                9   \n",
       "3        Women Tshirts                8   \n",
       "4  Women Tops & Tunics               10   \n",
       "\n",
       "                                      Attribute_list  \n",
       "0  [color, neck, pattern, print_or_pattern_type, ...  \n",
       "1  [blouse_pattern, border, border_width, color, ...  \n",
       "2  [color, fit_shape, length, occasion, ornamenta...  \n",
       "3  [color, fit_shape, length, pattern, print_or_p...  \n",
       "4  [color, fit_shape, length, neck_collar, ocassi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "category_attributes['Attribute_list'] = category_attributes['Attribute_list'].apply(\n",
    "    lambda x: [attr.lower() for attr in x]\n",
    ")\n",
    "category_attributes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b8de1f-371e-4a33-92ad-75800a9ad2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pre-trained model normalization\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image)\n",
    "\n",
    "train_image_folder = \"train_images/\"\n",
    "test_image_folder = \"test_images/\"\n",
    "\n",
    "preprocessed_train_images = {\n",
    "    str(img_id): preprocess_image(os.path.join(train_image_folder, f\"{img_id:06d}.jpg\"))\n",
    "    for img_id in train_data['id'][:2000]\n",
    "}\n",
    "\n",
    "preprocessed_test_images = {\n",
    "    str(img_id): preprocess_image(os.path.join(test_image_folder, f\"{img_id:06d}.jpg\"))\n",
    "    for img_id in test_data['id'][:750]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6822e96-9109-4180-99f2-0c424c94dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define the model with added novelty\n",
    "class AttributeClassifier(nn.Module):\n",
    "    def __init__(self, num_attributes, num_categories):\n",
    "        super(AttributeClassifier, self).__init__()\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        num_features = self.feature_extractor.fc.in_features\n",
    "        self.feature_extractor.fc = nn.Identity()  \n",
    "\n",
    "        embedding_dim = 50\n",
    "        self.category_embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "\n",
    "        self.attribute_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(num_features + embedding_dim, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5),  # Dropout layer for regularization\n",
    "                nn.Linear(128, 1)\n",
    "            ) for _ in range(num_attributes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, images, categories):\n",
    "        features = self.feature_extractor(images)\n",
    "        category_embeds = self.category_embedding(categories)\n",
    "        combined = torch.cat([features, category_embeds], dim=1)\n",
    "        \n",
    "        # Use a residual connection for the outputs\n",
    "        outputs = torch.cat([head(combined).squeeze(1).unsqueeze(1) for head in self.attribute_heads], dim=1)\n",
    "        return torch.sigmoid(outputs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c162da95-8fc6-4840-90dd-6869e755cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_attributes = 77  # Update this based on your dataset\n",
    "num_categories = len(subset_train_data['Category'].unique())  # Extracted dynamically from dataset\n",
    "model = AttributeClassifier(num_attributes, num_categories).to(device)\n",
    "\n",
    "# Evaluation metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_f1_scores(y_true, y_pred):\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    macro_f1 = f1_score(y_true, y_pred_binary, average='macro',zero_division=1)\n",
    "    micro_f1 = f1_score(y_true, y_pred_binary, average='micro',zero_division=1)\n",
    "    return macro_f1, micro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae4221-de1f-42bb-a686-df5f95776728",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3b0d667-e4fa-46a0-90b3-575d9fda198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb0ddc4-5759-4678-a6a2-349e98f35077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['a-line' 'applique' 'big border' 'black' 'blue' 'botanical' 'boxy'\n",
      " 'calf length' 'casual' 'checked' 'cream' 'crop' 'cuffed sleeves' 'daily'\n",
      " 'default' 'elephant' 'ethnic motif' 'fitted' 'floral' 'funky print'\n",
      " 'graphic' 'green' 'grey' 'high' 'jacquard' 'knee length' 'knitted' 'long'\n",
      " 'long sleeves' 'loose' 'maroon' 'multicolor' 'navy blue' 'net' 'no'\n",
      " 'no border' 'orange' 'party' 'peach' 'peacock' 'pink' 'polo' 'printed'\n",
      " 'puff sleeves' 'purple' 'quirky' 'red' 'regular' 'regular sleeves'\n",
      " 'round' 'round neck' 'ruffles' 'same as border' 'same as saree'\n",
      " 'short sleeves' 'sleeveless' 'small border' 'solid' 'square neck'\n",
      " 'straight' 'stylised' 'sweetheart neck' 'tassels and latkans'\n",
      " 'temple border' 'three-quarter sleeves' 'tie-ups' 'traditional'\n",
      " 'typography' 'v-neck' 'waist tie-ups' 'wedding' 'white' 'woven design'\n",
      " 'yellow' 'yes' 'zari' 'zari woven']\n",
      "Encoded labels shape: (70213, 77)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Filter only attribute columns (attr_1 to attr_10)\n",
    "attribute_cols = [col for col in train_data.columns if col.startswith('attr_')]\n",
    "\n",
    "# Preprocessing function for each row\n",
    "def preprocess_attributes(row):\n",
    "    return [str(x).strip() for x in row if pd.notnull(x) and str(x).strip()]\n",
    "\n",
    "# Apply row-wise to create lists of strings\n",
    "formatted_labels = train_data[attribute_cols].apply(preprocess_attributes, axis=1)\n",
    "\n",
    "# Combine all unique attributes across rows for binarization\n",
    "unique_classes = sorted(set(attr for row in formatted_labels for attr in row))\n",
    "\n",
    "# Use MultiLabelBinarizer with the unique classes\n",
    "mlb = MultiLabelBinarizer(classes=unique_classes)\n",
    "encoded_labels = mlb.fit_transform(formatted_labels)\n",
    "\n",
    "# Confirm the output\n",
    "encoded_df = pd.DataFrame(encoded_labels, columns=mlb.classes_)\n",
    "train_data_encoded = pd.concat([train_data.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(f\"Classes: {mlb.classes_}\")\n",
    "print(f\"Encoded labels shape: {encoded_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1110d35e-da51-4ffd-bd96-a3d11b8350f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [default, round, printed, default, short sleev...\n",
       "1        [multicolor, polo, solid, solid, short sleeves...\n",
       "2        [default, polo, solid, solid, short sleeves, s...\n",
       "3        [multicolor, polo, solid, solid, short sleeves...\n",
       "4        [multicolor, polo, solid, solid, short sleeves...\n",
       "                               ...                        \n",
       "70208    [multicolor, fitted, regular, square neck, cas...\n",
       "70209    [yellow, regular, crop, round neck, casual, de...\n",
       "70210    [maroon, fitted, crop, round neck, casual, sol...\n",
       "70211    [default, regular, regular, high, casual, shor...\n",
       "70212    [pink, boxy, crop, v-neck, casual, printed, ty...\n",
       "Length: 70213, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "763aa448-da0e-4075-af16-0263e8d447b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDataset(Dataset):\n",
    "    def __init__(self, dataframe, preprocessed_images, encoded_labels, category_encoder):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe: DataFrame containing metadata (e.g., ID, Category).\n",
    "            preprocessed_images: Dictionary mapping image IDs to tensors.\n",
    "            encoded_labels: Encoded attribute labels (e.g., numpy array or tensor).\n",
    "            category_encoder: Encoder for converting categories to numerical labels.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.preprocessed_images = preprocessed_images\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.category_encoder = category_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve metadata row\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Load the image\n",
    "        img_id = str(row['id'])  # Ensure ID is string to match dictionary keys\n",
    "        if img_id in self.preprocessed_images:\n",
    "            image = self.preprocessed_images[img_id]\n",
    "        else:\n",
    "            # Log missing image and use a placeholder tensor\n",
    "            print(f\"Image not found for ID: {img_id}. Using dummy tensor.\")\n",
    "            image = torch.zeros(3, 224, 224)  # Dummy tensor for missing images\n",
    "\n",
    "        # Encode the category\n",
    "        category = row['Category']\n",
    "        category = torch.tensor(self.category_encoder.transform([category])[0], dtype=torch.long)\n",
    "\n",
    "        # Load the pre-encoded attribute labels\n",
    "        labels = torch.tensor(self.encoded_labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, category, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fe73223-6a7f-42d6-8771-46a1c61eff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_data = train_data.head(2000) \n",
    "subset_encoded_labels = encoded_labels[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a92a6b9-4ad3-48ea-ad45-323e7ca2c984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(subset_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310981b2-81a5-4995-9282-6bfc9b8ad409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 2000\n",
      "Size of train loader (number of batches): 63\n"
     ]
    }
   ],
   "source": [
    "# Use all training data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "category_encoder=LabelEncoder()\n",
    "category_encoder.fit(subset_train_data['Category'])\n",
    "train_dataset = AttributeDataset(subset_train_data,preprocessed_train_images, encoded_labels,category_encoder=category_encoder)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check the size of the dataset and loader\n",
    "print(\"Size of train dataset:\", len(train_dataset))\n",
    "print(\"Size of train loader (number of batches):\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59d3266b-283f-4bd0-9859-7276957cdd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Men Tshirts': 0, 'Sarees': 1, 'Kurtis': 2, 'Women Tshirts': 3, 'Women Tops & Tunics': 4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Get unique categories from the 'Category' column and create a mapping to indices\n",
    "category_to_index = {category: idx for idx, category in enumerate(train_df['Category'].unique())}\n",
    "print(category_to_index)  # This will show the actual category to index mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfcd270-f86d-4bee-86fc-7d335a5a9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edf48d46-0f06-41a4-aa75-21c7167c6911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanav-arora/Desktop/AI/path/to/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kanav-arora/Desktop/AI/path/to/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3335119594657232, Macro F1: 0.10175008783322144, Micro F1: 0.21120347269615028\n",
      "Epoch 2/5, Loss: 0.3262584696686457, Macro F1: 0.11650526760244438, Micro F1: 0.24549554924468414\n",
      "Epoch 3/5, Loss: 0.3196418441477276, Macro F1: 0.12414578979555049, Micro F1: 0.2752946578907087\n",
      "Epoch 4/5, Loss: 0.31274342678842093, Macro F1: 0.12901821398417818, Micro F1: 0.31143156501599206\n",
      "Epoch 5/5, Loss: 0.3042297533580235, Macro F1: 0.13261293400466495, Micro F1: 0.35593582887700537\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "# Define parameters\n",
    "num_attributes = 77  # Update this based on your dataset\n",
    "num_categories = len(subset_train_data['Category'].unique())  # Extracted dynamically from dataset\n",
    "model = AttributeClassifier(num_attributes, num_categories).to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.BCEWithLogitsLoss()(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss\n",
    "        \n",
    "criterion = FocalLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# Training loop\n",
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Unpack the batch\n",
    "        images, categories, labels = batch\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        images, categories, labels = images.to(device), categories.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(images, categories)  # Forward pass\n",
    "        loss = criterion(outputs, labels.float())  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_outputs.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    # Compute F1 scores after each epoch\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    macro_f1, micro_f1 = compute_f1_scores(all_labels, all_outputs)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}, Macro F1: {macro_f1}, Micro F1: {micro_f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3888cca8-e6fb-44bd-a49b-bdbc5121ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ('Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts', 'Men Tshirts'), Type: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Categories: {categories}, Type: {type(categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc263020-34d8-4c00-9b10-b0049649fb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.]])\n",
      "a-line: Absent\n",
      "applique: Absent\n",
      "big border: Absent\n",
      "black: Absent\n",
      "blue: Absent\n",
      "botanical: Absent\n",
      "boxy: Absent\n",
      "calf length: Absent\n",
      "casual: Absent\n",
      "checked: Absent\n",
      "cream: Absent\n",
      "crop: Absent\n",
      "cuffed sleeves: Absent\n",
      "daily: Absent\n",
      "default: Present\n",
      "elephant: Absent\n",
      "ethnic motif: Absent\n",
      "fitted: Absent\n",
      "floral: Absent\n",
      "funky print: Absent\n",
      "graphic: Absent\n",
      "green: Absent\n",
      "grey: Absent\n",
      "high: Absent\n",
      "jacquard: Absent\n",
      "knee length: Absent\n",
      "knitted: Absent\n",
      "long: Absent\n",
      "long sleeves: Absent\n",
      "loose: Absent\n",
      "maroon: Absent\n",
      "multicolor: Absent\n",
      "navy blue: Absent\n",
      "net: Absent\n",
      "no: Absent\n",
      "no border: Absent\n",
      "orange: Absent\n",
      "party: Present\n",
      "peach: Absent\n",
      "peacock: Absent\n",
      "pink: Absent\n",
      "polo: Absent\n",
      "printed: Present\n",
      "puff sleeves: Absent\n",
      "purple: Absent\n",
      "quirky: Absent\n",
      "red: Absent\n",
      "regular: Present\n",
      "regular sleeves: Absent\n",
      "round: Present\n",
      "round neck: Absent\n",
      "ruffles: Absent\n",
      "same as border: Absent\n",
      "same as saree: Absent\n",
      "short sleeves: Absent\n",
      "sleeveless: Absent\n",
      "small border: Absent\n",
      "solid: Absent\n",
      "square neck: Absent\n",
      "straight: Absent\n",
      "stylised: Absent\n",
      "sweetheart neck: Absent\n",
      "tassels and latkans: Absent\n",
      "temple border: Absent\n",
      "three-quarter sleeves: Absent\n",
      "tie-ups: Absent\n",
      "traditional: Absent\n",
      "typography: Absent\n",
      "v-neck: Absent\n",
      "waist tie-ups: Absent\n",
      "wedding: Absent\n",
      "white: Absent\n",
      "woven design: Absent\n",
      "yellow: Absent\n",
      "yes: Absent\n",
      "zari: Absent\n",
      "zari woven: Absent\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the preprocessing transformation (adjust based on model requirements)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet stats\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "test_image_path = '/home/kanav-arora/Desktop/AI/test_images/000008.jpg'\n",
    "test_image = Image.open(test_image_path).convert('RGB')  # Ensure it's RGB format\n",
    "\n",
    "# Preprocess the image\n",
    "test_image = preprocess(test_image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Assume the category is known (replace with actual category index)\n",
    "category_index = 0  # Replace with actual category index (you can map category names to indices)\n",
    "\n",
    "# Convert the category to a tensor\n",
    "category_tensor = torch.tensor([category_index], dtype=torch.long).to(device)\n",
    "\n",
    "# Switch to evaluation mode and make prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # No need to calculate gradients during inference\n",
    "    output = model(test_image, category_tensor)  # Make prediction\n",
    "    probability = torch.sigmoid(output)  # Apply sigmoid to logits to get probabilities\n",
    "    prediction = (probability > 0.65).float()  # Binarize the output based on threshold (0.5)\n",
    "print(prediction)\n",
    "\n",
    "predicted_attributes = prediction.squeeze().tolist()  # Convert to list\n",
    "attributes = [\n",
    "    'a-line', 'applique', 'big border', 'black', 'blue', 'botanical', 'boxy',\n",
    "    'calf length', 'casual', 'checked', 'cream', 'crop', 'cuffed sleeves', 'daily',\n",
    "    'default', 'elephant', 'ethnic motif', 'fitted', 'floral', 'funky print',\n",
    "    'graphic', 'green', 'grey', 'high', 'jacquard', 'knee length', 'knitted', 'long',\n",
    "    'long sleeves', 'loose', 'maroon', 'multicolor', 'navy blue', 'net', 'no',\n",
    "    'no border', 'orange', 'party', 'peach', 'peacock', 'pink', 'polo', 'printed',\n",
    "    'puff sleeves', 'purple', 'quirky', 'red', 'regular', 'regular sleeves',\n",
    "    'round', 'round neck', 'ruffles', 'same as border', 'same as saree',\n",
    "    'short sleeves', 'sleeveless', 'small border', 'solid', 'square neck',\n",
    "    'straight', 'stylised', 'sweetheart neck', 'tassels and latkans',\n",
    "    'temple border', 'three-quarter sleeves', 'tie-ups', 'traditional',\n",
    "    'typography', 'v-neck', 'waist tie-ups', 'wedding', 'white', 'woven design',\n",
    "    'yellow', 'yes', 'zari', 'zari woven'\n",
    "]\n",
    "# Print each attribute with its prediction\n",
    "for name, value in zip(attributes, predicted_attributes):\n",
    "    print(f\"{name}: {'Present' if value == 1 else 'Absent'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ebc78ac-3a28-4c2c-b56f-e272eedcfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming categories is a list of category strings\n",
    "if isinstance(categories, tuple):\n",
    "    # Convert each category string to its corresponding index\n",
    "    category_indices = [category_to_index[cat] for cat in categories]\n",
    "    categories = torch.tensor(category_indices, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41616b7-9606-413e-ba96-14306c9cf81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile\n",
    "import io\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    image = Image.open(io.BytesIO(await file.read())).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).cuda()  # Preprocess\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "    predictions = outputs.cpu().numpy()\n",
    "    return {\"attributes\": predictions.tolist()}\n",
    "\n",
    "# Run with: uvicorn filename:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe00eb-978f-4f7b-81bc-3a37f081e173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a5f9b-2941-4c0f-b15c-190936a871ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
